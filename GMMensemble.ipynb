{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "617de6d0",
   "metadata": {},
   "source": [
    "# GMM ensemble\n",
    "Created by: Lea Poropat\n",
    "\n",
    "Last edit: 2023/06/23\n",
    "\n",
    "Unsupervised classification of the ocean based on satellite altimetry data with an ensemble of Gaussian Mixture Models (GMMs). The ensemble consists of N models, each fitted to K mixture components or classes. The classes from all ensemble members are matched, considering them the same class if the correlation between class means is higher than the correlation cutoff. Then for each grid point, we use soft voting, i.e., calculate the sum of probabilities for each class that all ensemble members provided, and the grid point is assigned the class with the highest sum of probabilities. Likelihood is calculated by dividing this highest sum of probabilities with the number of ensemble members N. The algorithm then checks if any classes are too small (smaller than the minimal size given) and removes those classes, re-assigning the grid point the next best class. There is also a possibility to only use votes with high probabilities by setting the minproba parameter.\n",
    "\n",
    "A small subset of data is saved as a test set (~10%), while the training set is randomly drawn from the rest of it. test_size refers to the size of the test set, while train_size refers to the percentage of the whole data set randomly taken from the remaining data. E.g., test_size = 10%, train_size = 30% means that 10% is taken aside for testing, while the training set is a random third (33%) of the remaining data set.\n",
    "\n",
    "Input data is in .npy format. The models needs 3 files: EOF maps (nEOFs x nlat x nlon), longitude grid, and latitude grid (both nlat x nlon). The results are saved in .mat format. The name of the results file is created based on model and ensemble parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac85de6",
   "metadata": {},
   "source": [
    "### <font color = \"red\">Parameters</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3228b9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region\n",
    "reg = 'North'\n",
    "\n",
    "# number of PCs used\n",
    "nPC = 3\n",
    "\n",
    "# file names\n",
    "pth = r'Files/' + reg + '_'\n",
    "fleof = pth + 'eof_maps.npy'\n",
    "fllon = pth + 'Lon.npy'\n",
    "fllat = pth + 'Lat.npy'\n",
    "\n",
    "# percent of training set\n",
    "test_size = 0.1\n",
    "train_size = 0.9\n",
    "\n",
    "# number of classes in the GMM\n",
    "K = 6\n",
    "\n",
    "# correlation cutoff\n",
    "corrlim = 0.98\n",
    "\n",
    "# minimal probability to take into account a vote (0 = all probabilities are taken into account)\n",
    "minproba = 0.0\n",
    "\n",
    "# minimal allowed class size (number of gridpoints in combined train and test set)\n",
    "minsize = 100\n",
    "\n",
    "# number of ensemble members\n",
    "N = 200\n",
    "\n",
    "# number of the same model\n",
    "no = 1\n",
    "\n",
    "# model name\n",
    "modelname = reg + '_PCs' + str(nPC) + \\\n",
    "    '_tr' + str(int(train_size*100)) + \\\n",
    "    '_K' + str(K) + '_r' + \\\n",
    "    str(int(corrlim*100)).zfill(2) + \\\n",
    "    '_p' + str(int(minproba*100)).zfill(2) + \\\n",
    "    '_gp' + str(minsize) + \\\n",
    "    '_N' + str(N) + \\\n",
    "    '_no' + str(no)\n",
    "\n",
    "# results\n",
    "res = r'Files/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe2deeb",
   "metadata": {},
   "source": [
    "### Loading the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35c69d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xporol\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\xporol\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\xporol\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "#import os\n",
    "#import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.stats import pearsonr\n",
    "import scipy.io\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902d7c7d",
   "metadata": {},
   "source": [
    "### Loading and reshaping data + removing empty grid points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64ca804a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Land grid points and grid points with missing data removed!\n",
      "Shape of the dataset after removing empty points: (2381, 300)\n"
     ]
    }
   ],
   "source": [
    "# loading the data\n",
    "X0 = np.load(fleof)\n",
    "Lon0 = np.load(fllon)\n",
    "Lat0 = np.load(fllat)\n",
    "nlat, nlon = np.shape(Lon0)\n",
    "\n",
    "# 1D longitude and latitude\n",
    "lon = Lon0[0, :]\n",
    "lat = Lat0[:, 0]\n",
    "\n",
    "# reshape to 2D\n",
    "X1 = np.reshape(X0,(len(X0),-1),'C').transpose()\n",
    "Lon1 = np.reshape(Lon0, -1, 'C').transpose()\n",
    "Lat1 = np.reshape(Lat0, -1, 'C').transpose()\n",
    "\n",
    "# remove land and empty grid points\n",
    "ind_ocean = np.squeeze(np.nonzero(np.squeeze(~np.all(np.isnan(X1), axis = 1))))\n",
    "X2 = X1[ind_ocean, :]\n",
    "Lon2 = Lon1[ind_ocean]\n",
    "Lat2 = Lat1[ind_ocean]\n",
    "\n",
    "# check if there are any gaps in the dataset\n",
    "gaps = np.argwhere(np.any(np.isnan(X2), axis = 1))\n",
    "gaps = np.squeeze(gaps)\n",
    "\n",
    "# printing the summary\n",
    "if len(gaps)==0:\n",
    "    print('Land grid points and grid points with missing data removed!')\n",
    "print('Shape of the dataset after removing empty points: %s' % str(np.shape(X2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d606bfea",
   "metadata": {},
   "source": [
    "### Selecting the principal components that will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07e38341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of PCs used:   3\n"
     ]
    }
   ],
   "source": [
    "# select the PCs that combined make up {cutoff}% of variability\n",
    "X = X2[:, :nPC]\n",
    "npcs = np.shape(X)[1]\n",
    "\n",
    "# print the number of PCs used\n",
    "print('Number of PCs used: %3i' % (nPC))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf79aa4",
   "metadata": {},
   "source": [
    "### Train-test set split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63c4ff47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting a small subset of data that will be kept separately for testing\n",
    "ind = np.array(range(0, len(X)))\n",
    "\n",
    "Xtr, Xte, Lontr, Lonte, Lattr, Latte, indtr, indte = train_test_split(X, Lon2, Lat2, ind, test_size = test_size)\n",
    "ntr = np.shape(Xtr)[0]\n",
    "nte = np.shape(Xte)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab7d956",
   "metadata": {},
   "source": [
    "### GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b9ab82f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 200  199  198  197  196  195  194  193  192  191  190  189  188  187  186  185  184  183  182  181  180  179  178  177  176\n",
      " 175  174  173  172  171  170  169  168  167  166  165  164  163  162  161  160  159  158  157  156  155  154  153  152  151\n",
      " 150  149  148  147  146  145  144  143  142  141  140  139  138  137  136  135  134  133  132  131  130  129  128  127  126\n",
      " 125  124  123  122  121  120  119  118  117  116  115  114  113  112  111  110  109  108  107  106  105  104  103  102  101\n",
      " 100   99   98   97   96   95   94   93   92   91   90   89   88   87   86   85   84   83   82   81   80   79   78   77   76\n",
      "  75   74   73   72   71   70   69   68   67   66   65   64   63   62   61   60   59   58   57   56   55   54   53   52   51\n",
      "  50   49   48   47   46   45   44   43   42   41   40   39   38   37   36   35   34   33   32   31   30   29   28   27   26\n",
      "  25   24   23   22   21   20   19   18   17   16   15   14   13   12   11   10    9    8    7    6    5    4    3    2    1\n"
     ]
    }
   ],
   "source": [
    "# creating the results matrices\n",
    "y = np.empty((ntr+nte, N), dtype = np.int16)\n",
    "proba = np.empty((ntr+nte, N))\n",
    "class_means = np.empty((K, npcs, N))\n",
    "\n",
    "for i in range(N):\n",
    "    # creating the model\n",
    "    gmm = GaussianMixture(n_components = K, covariance_type = 'full', tol = 1e-3, \\\n",
    "                          max_iter = 200, n_init = 3, init_params = 'kmeans', verbose = 0)\n",
    "    \n",
    "    # drawing a random subset of the whole training to use for training of this ensemble member\n",
    "    # the remaining data points will also be classified\n",
    "    indtemp = np.arange(0, ntr)\n",
    "    Xtrs, Xtrr, Lontrs, Lontrr, Lattrs, Lattrr, indtrs, indtrr = train_test_split(Xtr, Lontr, Lattr, indtemp, test_size = (1-train_size))\n",
    "    ntrs = np.shape(Xtrs)[0]\n",
    "    ntrr = np.shape(Xtrr)[0]\n",
    "    \n",
    "    # fitting the model\n",
    "    gmm.fit(Xtrs)\n",
    "    \n",
    "    # predicting the class for each grid point\n",
    "    ytrs = gmm.predict(Xtrs)\n",
    "    ytrr = gmm.predict(Xtrr)\n",
    "    yte = gmm.predict(Xte)\n",
    "    \n",
    "    # predicting the probability for each point\n",
    "    temp = gmm.predict_proba(Xtrs)\n",
    "    probatrs = np.amax(temp, axis = 1)\n",
    "    temp = gmm.predict_proba(Xtrr)\n",
    "    probatrr = np.amax(temp, axis = 1)\n",
    "    temp = gmm.predict_proba(Xte)\n",
    "    probate = np.amax(temp, axis = 1)\n",
    "    \n",
    "    # saving the class means\n",
    "    class_means[:, :, i] = gmm.means_\n",
    "    \n",
    "    # combining this ensemble member's train and remaining set\n",
    "    ytr = np.empty((ntr), dtype = np.int16)\n",
    "    ytr[indtrs] = ytrs\n",
    "    ytr[indtrr] = ytrr\n",
    "    probatr = np.empty((ntr))\n",
    "    probatr[indtrs] = probatrs\n",
    "    probatr[indtrr] = probatrr\n",
    "    \n",
    "    # combining the train and test set again\n",
    "    y[indtr, i] = ytr\n",
    "    y[indte, i] = yte\n",
    "    proba[indtr, i] = probatr\n",
    "    proba[indte, i] = probate\n",
    "    \n",
    "    # printing the ensemble number\n",
    "    if ((N-i-1) % 25 == 0):\n",
    "        print('%4i' % (N-i))\n",
    "    else:\n",
    "        print('%4i' % (N-i), end = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684ae524",
   "metadata": {},
   "source": [
    "### Combining the results from the ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d410cad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSizeOfNestedList(listOfElem):\n",
    "    ''' Get number of elements in a nested list'''\n",
    "    count = 0\n",
    "    # Iterate over the list\n",
    "    for elem in listOfElem:\n",
    "        # Check if type of element is list\n",
    "        if type(elem) == list:  \n",
    "            # Again call this function to get the size of this element\n",
    "            count += getSizeOfNestedList(elem)\n",
    "        else:\n",
    "            count += 1    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bef3320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the list of all classes that appear in any of the models (classes with corr >= corrlim are considered same)\n",
    "# the first N classes are from the 1st ensemble member\n",
    "classes_all = np.expand_dims(class_means[0, :, 0], 0)\n",
    "classes_matching = np.empty((K, N), dtype = np.int64)\n",
    "classes_matching[0, 0] = 0\n",
    "class_means_new = np.expand_dims(class_means[0, :, 0], (0, 1)).tolist()\n",
    "\n",
    "num = getSizeOfNestedList(class_means_new)\n",
    "\n",
    "# comparing the class means from the other ensemble members with the growing list of separate classes\n",
    "for i in range(0, N):\n",
    "    for j in range(K):\n",
    "        \n",
    "        if ((i == 0) and (j == 0)):\n",
    "            continue\n",
    "        \n",
    "        r = np.empty((np.shape(classes_all)[0]))\n",
    "        \n",
    "        for k in range(len(r)):\n",
    "            r[k] = pearsonr(class_means[j, :, i], classes_all[k, :])[0]\n",
    "            \n",
    "            \n",
    "        if (np.amax(r) >= corrlim):\n",
    "            temp = np.argmax(r)\n",
    "            classes_matching[j, i] = temp\n",
    "            class_means_new[temp].append(class_means[j, :, i].tolist())\n",
    "        else:\n",
    "            classes_all = np.concatenate((classes_all, np.reshape(class_means[j, :, i], (1, npcs))))\n",
    "            classes_matching[j, i] = np.shape(classes_all)[0]-1\n",
    "            class_means_new.append(np.expand_dims(class_means[j, :, i], 0).tolist())\n",
    "    \n",
    "    # finding and printing the number of class means after each ensemble member is sorted\n",
    "    temp = num\n",
    "    num = getSizeOfNestedList(class_means_new)\n",
    "    #print('%2i -> %4i | %3i' % (i, num, (num-temp)/8))\n",
    "            \n",
    "tot_class_num = np.shape(classes_all)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99476a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning new class numbers to the results\n",
    "yNew = np.empty((ntr+nte, N), dtype = np.int64)\n",
    "\n",
    "for i in range(N):    \n",
    "    for j in range(K):\n",
    "        ind = np.squeeze(np.nonzero(y[:, i] == j))\n",
    "        yNew[ind, i] = classes_matching[j, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9cd9ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# majority voting + likelihood (probability cutoff + weighted votes + removing small classes)\n",
    "def soft_vote(x, proba, nan = -99, minproba = 0.5, minclass = 100):\n",
    "    # array with samples as rows and ensemble models as columns\n",
    "    \n",
    "    # maority voting for the whole array\n",
    "    x2 = np.empty((np.shape(x)[0]), dtype = np.int16)\n",
    "    like = np.empty((np.shape(x)[0]))\n",
    "    votes = []\n",
    "    \n",
    "    for i in range(np.shape(x)[0]):\n",
    "        mask = np.nonzero((x[i, :] == nan) & (proba[i, :] < minproba))        \n",
    "        classes = np.delete(x[i, :], mask) # removing empty and too weak votes\n",
    "        weights = np.delete(proba[i, :], mask)\n",
    "        votes.append(np.bincount(classes, weights = weights)) # number of votes for each class\n",
    "        \n",
    "        if len(votes[i]) == 0:\n",
    "            x2[i] = -1\n",
    "            like[i] = 0\n",
    "        else:\n",
    "            x2[i] = votes[i].argmax()       # class with the highest vote\n",
    "            like[i] = votes[i].max()/np.shape(x)[1] # how many voted for this class\n",
    "            \n",
    "    # check if some of the classes are too small - if yes, use the next best not-too-small class for the grid points\n",
    "    # that belong to the small classes\n",
    "    \n",
    "    # finding small classes\n",
    "    maxclass = np.max(x)\n",
    "    class_size = np.empty((maxclass+1), dtype = np.int32)\n",
    "    for i in range(maxclass + 1):\n",
    "        class_size[i] = len(np.nonzero(x2 == i)[0])\n",
    "        \n",
    "    # small classes\n",
    "    small_classes = np.nonzero(class_size <= minclass)[0]\n",
    "    \n",
    "    # finding a new class for each of the grid points belonging to a small class\n",
    "    for i in range(np.shape(x2)[0]):\n",
    "        if np.isin(x2[i], small_classes, assume_unique = True):     #x2[i] in small_classes:\n",
    "            # deleting the votes that blong to the small classes (temp = votes from only large enough classes)\n",
    "            temp = np.zeros((len(votes[i])), dtype = np.int16)\n",
    "            for k in range(len(temp)):\n",
    "                if not np.isin(k, small_classes, assume_unique = True):  # k not in small_classes:\n",
    "                    temp[k] = votes[i][k]\n",
    "            \n",
    "            # choosing the second best vote (or -1 if there are no other votes)\n",
    "            #print((i, x2[i]), end = ' -> ')\n",
    "            if np.any(temp):  # if any elements is not zero\n",
    "                x2[i] = temp.argmax()\n",
    "            else:\n",
    "                x2[i] = -1\n",
    "            #print(x2[i])\n",
    "            like[i] = temp.max()/np.shape(x)[1]\n",
    "            \n",
    "    return [x2, like]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c243bc6d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# majority voting\n",
    "ySV, like = soft_vote(yNew, proba, nan = -99, minproba = minproba, minclass = minsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a5b141d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class  0:      319\n",
      "class  1:        0\n",
      "class  2:      313\n",
      "class  3:      315\n",
      "class  4:      287\n",
      "class  5:      430\n",
      "class  6:      717\n",
      "class  7:        0\n",
      "class  8:        0\n"
     ]
    }
   ],
   "source": [
    "# how many grid points exist in each class\n",
    "points_per_class = np.empty((tot_class_num))\n",
    "for i in range(tot_class_num):\n",
    "    points_per_class[i] = len(np.squeeze(np.nonzero(ySV == i)))\n",
    "    print('class %2i: %8i' % (i, points_per_class[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13e26d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of classes:  9\n",
      "Final number of classes:  6\n"
     ]
    }
   ],
   "source": [
    "# dropping the empty classes\n",
    "classes = np.arange(tot_class_num)\n",
    "ind = np.squeeze(points_per_class > 0)\n",
    "classes = classes[ind]\n",
    "new_class_num = len(classes)\n",
    "classes_new = np.squeeze(classes_all[ind, :])\n",
    "class_means_new = [a for (a, m) in zip(class_means_new, ind) if m]\n",
    "print('Total number of classes: %2i' % tot_class_num)\n",
    "print('Final number of classes: %2i' % new_class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cac5a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the class numbers to be from 1 to new_class_num (before the classes started at 0!)\n",
    "for i in range(new_class_num):\n",
    "    ySV[ySV == classes[i]] = i + 1001\n",
    "ySV = ySV - 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28114de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class  1:      319\n",
      "class  2:      313\n",
      "class  3:      315\n",
      "class  4:      287\n",
      "class  5:      430\n",
      "class  6:      717\n"
     ]
    }
   ],
   "source": [
    "# how many grid points exist in each class\n",
    "points_per_class = np.empty((new_class_num))\n",
    "for i in range(new_class_num):\n",
    "    points_per_class[i] = len(np.squeeze(np.nonzero(ySV == i+1)))\n",
    "    print('class %2i: %8i' % (i+1, points_per_class[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2452c650",
   "metadata": {},
   "source": [
    "### Saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b37ae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the data back to a grid\n",
    "res_grid = np.nan * np.ones((nlat*nlon), dtype = np.int16)\n",
    "res_grid[ind_ocean] = np.squeeze(ySV)\n",
    "res_grid = np.reshape(res_grid, (nlat, nlon),'C')\n",
    "\n",
    "like_grid = np.nan * np.ones((nlat*nlon))\n",
    "like_grid[ind_ocean] = np.squeeze(like)\n",
    "like_grid = np.reshape(like_grid, (nlat, nlon), 'C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d4cb8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class means from voting (special treatment because it is not a regular np array that can be converted to matlab array)\n",
    "ensemble_means = dict()\n",
    "\n",
    "# creating a dictionary with class number as key and a numpy aray of shape\n",
    "#     ensemble members that have this class x number of PCs\n",
    "# as values\n",
    "for i in range(new_class_num):\n",
    "    ensemble_means[str(i)] = np.array(class_means_new[i])\n",
    "    \n",
    "# saving the dictionary into a matlab readable file\n",
    "scipy.io.savemat(res+modelname+'ensemble_class_means.mat', ensemble_means)\n",
    "\n",
    "# all other regular variables\n",
    "# Lontr, Lattr = coordinates of the training set grid points\n",
    "# Lonte, Latte = coordinates of the test set grid points\n",
    "# lon, lat = coordinates in vector format\n",
    "# Lon2, Lat2, = coordinates of all results (combined train and test) in an array format\n",
    "# res_grid = classes for each grid point on a grid\n",
    "# like_grid = likelihood for each grid point on a grid\n",
    "# X = input data\n",
    "# ySV = results in array format\n",
    "scipy.io.savemat(res+modelname+'.mat', dict(Lontr = Lontr, Lattr = Lattr, \\\n",
    "                                            Lonte = Lonte, Latte = Latte, \\\n",
    "                                            lon = lon, lat = lat, \\\n",
    "                                            Lon = Lon2, Lat = Lat2, \\\n",
    "                                            res_grid = res_grid, \\\n",
    "                                            like_grid = like_grid, \\\n",
    "                                            X = X, \\\n",
    "                                            y = ySV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a8ad1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
